# PVD4RCV Dataset

*A Photo-realistic Multi-Distortion Video Dataset for Benchmarking and Developing Robust Computer Vision Models*


<p align="center">
  <img width="1491" height="860" alt="DatasetView" src="https://github.com/user-attachments/assets/c2de080a-5c42-4dba-b07c-d5dcdd4e0fad" />
</p>


## ğŸ“Œ Description

**PVD4RCV** (Photo-realistic Multi-Distortion Video Dataset for Robust Computer Vision) is a unique video database designed for evaluating and developing robust computer vision models.

Unlike traditional datasets where distortions are applied artificially, PVD4RCV incorporates **real physical factors** (scene depth, light interaction, motion dynamics) to generate photo-realistic distortions such as:

* Local and global motion blur
* Local and global defocus blur
* Compression and transmission artifacts
* Noise
* Contrast reduction
* Haze and smoke
* Depth-aware realistic rain

Each sequence is provided with depth maps and annotations to support tasks such as object detection, tracking, and distortion classification.

---

## ğŸ“‚ Dataset Content

* **24 original videos** (10 seconds each)
* **672 distorted videos** with 4 severity levels
* **Complete annotations**: object bounding boxes and labels
* **Associated depth maps**
* **Resolution**: 1920Ã—1080 (Full HD)
* **Frame rate**: 29.93 â€“ 30 fps
* **Format**: MP4
* **Total size**: ~21.2 GB

## ğŸ§© Dataset Split â€” Training / Validation / Test
| Set            | Number of Sequences | Percentage | Description                                                                      |
| :------------- | :-----------------: | :--------: | :------------------------------------------------------------------------------- |
| **Training**   |     16 sequences    |  **â‰ˆ 67%** | Used for model training â€” includes diverse environments and all distortion types |
| **Validation** |     4 sequences     |  **â‰ˆ 17%** | Used to tune hyperparameters and monitor model generalization                    |
| **Test**       |     4 sequences     |  **â‰ˆ 17%** | Held-out subset for final performance evaluation under unseen conditions         |

ğŸ—‚ Note:
Each subset contains both original and distorted versions of the videos (all 4 severity levels).
Splitting ensures scene disjointness â€” i.e., no identical scenes appear across training, validation, and test sets, ensuring a fair robustness evaluation.

### ğŸŒ Scenarios included

* Road traffic
* Parking lots
* Stadiums and crowds
* Airports
* Shopping malls
* Urban streets and train stations
* Sea navigation

---

## ğŸš€ Preparing the PVD4RCV Dataset

Before setting up the GroundTruth structure, make sure to download the complete dataset, which includes:

ğŸ§© Original and distorted videos (all 24 sequences Ã— 4 distortion levels Ã— multiple types)

ğŸŒˆ Depth maps associated with each video sequence

ğŸ—‚ï¸ Annotation files (bounding boxes, object classes, and distortion metadata)

You can download the full dataset package from the official link:
ğŸ‘‰ Download PVD4RCV Dataset

## ğŸ§° Step 1 â€” Extract video frames

Once the dataset is downloaded and extracted locally, run the provided Python script to extract frames from each video sequence and store them in structured folders.

Each frame will be automatically renamed following this convention: **{video_name}_{dist}_lvl{lvl}_{original_frame_name}**

Example:

```
traffic_rain_lvl2_frame_00045.jpg
airport_blur_lvl4_frame_00012.jpg
```

This ensures clear identification of the scene, distortion type, and severity level for every frame.

Run the script from the project root:

```
python extract_frames.py --input-dir ./PVD4RCV/Distorted --output-dir ./PVD4RCV/Frames
```

**What this script does:**
* Iterates through all distorted video sequences
* Extracts individual frames using OpenCV (cv2.VideoCapture)
* Saves them with the naming pattern above
* Preserves scene and distortion metadata for traceability

**Requirements:**
* Python â‰¥ 3.8
* Required libraries: opencv-python, tqdm, os, argparse
(install via pip install -r requirements.txt if needed)


## ğŸ—‚ï¸ Dataset Structure

The dataset is organized in a clear directory structure to facilitate access to original videos, distorted versions, and corresponding ground-truth data.
 addition to the video and depth data, **all annotations are provided in two complementary formats** to ensure compatibility with most computer vision frameworks:

- **COCO format**: JSON files following the COCO dataset structure, including bounding boxes, segmentation masks (if applicable), and category IDs consistent with the **COCO label indexing**.  
- **YOLO format**: Plain-text `.txt` files containing normalized bounding box coordinates and class indices following the **YOLO label convention**. Each video frame has its own corresponding `.txt` annotation file.

This dual-format annotation setup allows users to directly integrate PVD4RCV into common training pipelines such as **Detectron2**, **MMDetection**, or **Ultralytics YOLO** without additional preprocessing.


```
PVD4RCV/
â”‚
â”œâ”€â”€ Distorted/ # 672 distorted videos grouped by type & severity
â”‚ â”œâ”€â”€ Sequence1/
â”‚ â”‚ â”œâ”€â”€ Sequence1_DistortionType_DistorsionLevel.mp4
â”‚ â”‚ â””â”€â”€ ...
â”‚ â”œâ”€â”€ Sequence2/
â”‚ â”œâ”€â”€ Sequence3/
â”‚ â””â”€â”€ ../
â”‚
â”œâ”€â”€ GroundTruth/ # Truth value directory (annotations)
â”œâ”€â”€ Sequence1/ # video sequence
â”‚ â”œâ”€â”€ BoundingBoxes/ # Per-frame object annotations
â”‚ â”‚ â”œâ”€â”€ Sequence1.json
â”‚ â”‚ â”œâ”€â”€ txt/ # Per-frame object annotations
â”‚ â”‚ â”‚ â”œâ”€â”€ frame_000.txt
â”‚ â”‚ â”‚ â””â”€â”€ ...
â”‚ â”œâ”€â”€ depth/ # depth map annotations
â”‚ â”‚ â”œâ”€â”€ frame_000.png
â”‚ â”‚ â””â”€â”€ ...
â”‚
â”œâ”€â”€ ObjectClasses.txt # List of all object categories
â”œâ”€â”€ SceneMetadata.csv # Scene-level metadata (lighting, motion, etc.)
â””â”€â”€ DistortionLabels.csv # Ground-truth mapping: video â†” distortion type/level
```
---

### ğŸ“˜ Description of the *GroundTruth* folder

The **GroundTruth/** directory contains all the reference data used for model evaluation and training:

- **BoundingBoxes/** â†’ JSON files with per-frame bounding boxes and object IDs  
  *(format: frame, object_id, class, x_min, y_min, x_max, y_max)*  
- **ObjectClasses.txt** â†’ List of object classes present in the dataset (e.g. car, person, ball, etc.)  
- **SceneMetadata.csv** â†’ Global scene information such as lighting, motion dynamics, and environment type.  
- **DistortionLabels.csv** â†’ Mapping file linking each distorted video to its original reference and distortion parameters (type, severity, frame count).  

---

## ğŸ”§ Main Applications

PVD4RCV can be used for:

* **Robust Object Detection**: evaluating detection under degraded conditions
* **Visual Tracking**: testing robustness of tracking algorithms
* **Distortion Classification**: training/testing distortion recognition models
* **Scene Understanding**: benchmarking scene analysis in complex environments
* **Depth Estimation**: evaluating monocular depth estimation with ground-truth maps

---

## ğŸ“Š Key Features

âœ… Photo-realistic distortions based on physical models
âœ… Multiple severity levels for each distortion type
âœ… Large diversity of real-world scenarios
âœ… Includes annotations and depth maps
âœ… Suitable for benchmarking and training deep learning models

---


## ğŸ“Š Benchmark

Coming soon...

---

## ğŸ“¥ Access and Download

ğŸ‘‰ [Download Link] (insert official link here)

---

## ğŸ“œ Citation

If you use this dataset, please cite the associated paper:

```
@inproceedings{pvd4rcv2025,
  title={PVD4RCV: A Photo-realistic Multi-Distortion Video Dataset for Benchmarking and Developing Robust Computer Vision Models},
  booktitle={IEEE VCIP},
  year={2025}
}
```

---

## ğŸ‘¥ Authors & Contributions

* [List of article authors]
* Project presented at **IEEE VCIP 2025**

---

